---
name: Nova Ragie RAG Pipeline CI

on:
  push:
    branches: [main, master, development]
    paths:
      - "**.py"
      - "requirements.txt"
      - "pytest.ini"
      - ".github/workflows/nova-ragie.yml"
  pull_request:
    branches: [main, master, development]
    paths:
      - "**.py"
      - "requirements.txt"
      - "pytest.ini"
      - ".github/workflows/nova-ragie.yml"
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  issues: write
  checks: write

jobs:
  ragie-test-and-fix:
    name: Test Ragie RAG Pipeline with Nova
    runs-on: ubuntu-latest

    # Skip on Nova auto-fix commits (push only)
    if: >
      ${{ github.event_name != 'push' || (
            !contains(github.event.head_commit.message, '[skip nova]') &&
            !contains(github.event.head_commit.message, '🤖 Nova Auto-Fix')
          ) }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GH_TOKEN || secrets.GITHUB_TOKEN }}
          ref: ${{ github.event.pull_request.head.ref || github.ref }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install Ragie dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pytest pytest-json-report
          pip install ragie-python || echo "Ragie Python SDK not available"

      - name: Create Ragie test project
        run: |
          # Create a temporary Ragie project for testing
          mkdir -p ragie-test-workspace
          cd ragie-test-workspace

          # Create the Ragie RAG pipeline
          cat > ragie_retrieval.py << 'EOF'
          import numpy as np
          from typing import List, Dict, Any, Tuple
          import json

          def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
              """Compute cosine similarity between two vectors."""
              a_norm = a / np.linalg.norm(a)
              b_norm = b / np.linalg.norm(b)  # This will be broken in tests
              return np.dot(a_norm, b_norm)

          def rank(documents: List[str], query_emb: np.ndarray, doc_embeddings: List[np.ndarray], top_k: int = 5) -> List[int]:
              """Rank documents by similarity to query."""
              similarities = [cosine_sim(query_emb, doc_emb) for doc_emb in doc_embeddings]
              ranked_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)
              return ranked_indices[:top_k]  # This will be broken in tests

          def chunk_document(text: str, chunk_size: int = 100, overlap: int = 20) -> List[str]:
              """Split document into overlapping chunks."""
              chunks = []
              start = 0
              while start < len(text):
                  end = start + chunk_size
                  chunks.append(text[start:end])
                  start = end - overlap  # This will be broken in tests
              return chunks

          def compute_embedding_quality(embeddings: List[np.ndarray]) -> Dict[str, float]:
              """Compute quality metrics for embeddings."""
              norms = [np.linalg.norm(emb) for emb in embeddings]
              return {
                  "mean_norm": float(np.mean(norms)),
                  "variance": float(np.var(norms))  # This will be broken in tests
              }

          def optimize_retrieval_threshold(similarities: List[float], relevance: List[int]) -> float:
              """Find optimal threshold for retrieval using F1 score."""
              best_threshold = 0.5
              best_f1 = 0.0

              for threshold in np.arange(0.1, 1.0, 0.1):
                  predictions = [1 if sim >= threshold else 0 for sim in similarities]

                  tp = sum(p and r for p, r in zip(predictions, relevance))
                  fp = sum(p and not r for p, r in zip(predictions, relevance))
                  fn = sum(not p and r for p, r in zip(predictions, relevance))

                  precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                  recall = tp / (tp + fn) if (tp + fn) > 0 else 0

                  f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # This will be broken

                  if f1 > best_f1:
                      best_f1 = f1
                      best_threshold = threshold

              return best_threshold
          EOF

          # Create comprehensive tests
          cat > test_ragie_retrieval.py << 'EOF'
          import pytest
          import numpy as np
          from ragie_retrieval import cosine_sim, rank, chunk_document, compute_embedding_quality, optimize_retrieval_threshold

          class TestCosineSimNormalizedInvariant:
              """Test 1: Cosine similarity scale invariance - WILL FAIL due to missing normalization."""

              def test_cosine_sim_scale_invariant(self):
                  """Cosine similarity should be invariant to vector scaling."""
                  a = np.array([1.0, 2.0, 3.0])
                  b = np.array([4.0, 5.0, 6.0])

                  # Test with different scales
                  sim1 = cosine_sim(a, b)
                  sim2 = cosine_sim(a * 10, b)  # Scale vector a
                  sim3 = cosine_sim(a, b * 100)  # Scale vector b

                  # All should be approximately equal (scale invariant)
                  assert abs(sim1 - sim2) < 1e-10, f"Scale invariance failed: {sim1} != {sim2}"
                  assert abs(sim1 - sim3) < 1e-10, f"Scale invariance failed: {sim1} != {sim3}"

          class TestRankingConsistency:
              """Test 2: Document ranking consistency - WILL FAIL due to wrong order."""

              def test_ranking_descending_order(self):
                  """Rankings should be in descending order of similarity."""
                  documents = ["doc1", "doc2", "doc3", "doc4"]
                  query_emb = np.array([1.0, 0.0, 0.0])

                  # Create embeddings with known similarities
                  doc_embeddings = [
                      np.array([0.9, 0.1, 0.0]),  # High similarity
                      np.array([0.1, 0.9, 0.0]),  # Low similarity
                      np.array([0.8, 0.2, 0.0]),  # Medium-high similarity
                      np.array([0.2, 0.8, 0.0])   # Medium-low similarity
                  ]

                  ranked_indices = rank(documents, query_emb, doc_embeddings, top_k=4)

                  # Calculate actual similarities to verify order
                  similarities = [cosine_sim(query_emb, emb) for emb in doc_embeddings]
                  expected_order = sorted(range(len(similarities)),
                                        key=lambda i: similarities[i], reverse=True)

                  assert ranked_indices == expected_order, f"Wrong ranking order: {ranked_indices} != {expected_order}"

          class TestTopKSelection:
              """Test 3: Top-K selection accuracy - WILL FAIL due to off-by-one error."""

              def test_top_k_exact_count(self):
                  """Should return exactly top_k results, not top_k-1."""
                  documents = [f"doc{i}" for i in range(10)]
                  query_emb = np.array([1.0, 0.0])
                  doc_embeddings = [np.random.rand(2) for _ in range(10)]

                  for k in [1, 3, 5, 7]:
                      ranked = rank(documents, query_emb, doc_embeddings, top_k=k)
                      assert len(ranked) == k, f"Expected {k} results, got {len(ranked)}"

          class TestChunkingOverlap:
              """Test 4: Document chunking overlap - WILL FAIL due to wrong overlap calculation."""

              def test_chunking_overlap_consistency(self):
                  """Chunks should have consistent overlap as specified."""
                  text = "This is sentence one. This is sentence two. This is sentence three. " * 20
                  chunk_size = 100
                  overlap = 20

                  chunks = chunk_document(text, chunk_size=chunk_size, overlap=overlap)

                  # Verify overlap between consecutive chunks
                  for i in range(len(chunks) - 1):
                      chunk1_end = chunks[i][-overlap:]
                      chunk2_start = chunks[i + 1][:overlap]

                      # Should have some overlap (not exact due to word boundaries, but significant)
                      overlap_chars = len(set(chunk1_end.split()) & set(chunk2_start.split()))
                      assert overlap_chars > 0, f"No overlap found between chunks {i} and {i+1}"

          class TestEmbeddingQualityMetrics:
              """Test 5: Embedding quality calculation - WILL FAIL due to missing variance."""

              def test_embedding_quality_variance(self):
                  """Test that embedding quality includes variance calculation."""
                  embeddings = [
                      np.array([1.0, 2.0, 3.0]),
                      np.array([2.0, 3.0, 4.0]),
                      np.array([3.0, 4.0, 5.0])
                  ]

                  quality = compute_embedding_quality(embeddings)

                  # Should include variance calculation
                  assert quality["variance"] > 0.0

          class TestF1ScoreCalculation:
              """Test 6: F1 score formula correctness - WILL FAIL due to wrong F1 formula."""

              def test_f1_score_formula(self):
                  """F1 should be harmonic mean of precision and recall, not sum."""
                  similarities = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]
                  relevance = [1, 1, 1, 0, 0, 0, 0, 0]  # First 3 are relevant

                  threshold = optimize_retrieval_threshold(similarities, relevance)

                  # With threshold around 0.7, we should get precision=1.0, recall=1.0, F1=1.0
                  # But the broken F1 formula will give F1=2.0 (precision + recall)
                  predictions = [1 if sim >= threshold else 0 for sim in similarities]

                  tp = sum(p and r for p, r in zip(predictions, relevance))
                  fp = sum(p and not r for p, r in zip(predictions, relevance))
                  fn = sum(not p and r for p, r in zip(predictions, relevance))

                  precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                  recall = tp / (tp + fn) if (tp + fn) > 0 else 0

                  # Correct F1 formula (will fail with broken implementation)
                  expected_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

                  # The broken implementation returns precision + recall instead
                  assert expected_f1 <= 1.0, f"F1 score should be ≤ 1.0, got {expected_f1}"
          EOF

          # Create requirements.txt
          cat > requirements.txt << 'EOF'
          numpy>=1.21.0
          pytest>=7.0.0
          pytest-json-report>=1.5.0
          EOF

          # Create pytest.ini
          cat > pytest.ini << 'EOF'
          [tool:pytest]
          testpaths = .
          python_files = test_*.py
          python_classes = Test*
          python_functions = test_*
          addopts = -v --tb=short
          EOF

      - name: Introduce Ragie bugs
        run: |
          cd ragie-test-workspace

          # Bug 1: Remove normalization from cosine similarity
          sed -i 's/b_norm = b \/ np.linalg.norm(b)/b_norm = b/' ragie_retrieval.py

          # Bug 2: Change ranking order
          sed -i 's/reverse=True/reverse=False/' ragie_retrieval.py

          # Bug 3: Off-by-one error in top-k
          sed -i 's/\[:top_k\]/[:top_k-1]/' ragie_retrieval.py

          # Bug 4: Wrong overlap calculation
          sed -i 's/start = end - overlap/start = end - overlap + 50/' ragie_retrieval.py

          # Bug 5: Remove variance calculation
          sed -i 's/"variance": float(np.var(norms))/"variance": 0.0/' ragie_retrieval.py

          # Bug 6: Wrong F1 formula (sum instead of harmonic mean)
          sed -i 's/f1 = 2 \* (precision \* recall) \/ (precision + recall) if (precision + recall) > 0 else 0/f1 = precision + recall/' ragie_retrieval.py

      - name: Run Ragie tests
        id: run-tests
        continue-on-error: true
        run: |
          cd ragie-test-workspace
          pytest -v --json-report --json-report-file=test-results.json || TEST_EXIT_CODE=$?
          echo "test_exit_code=${TEST_EXIT_CODE:-0}" >> $GITHUB_OUTPUT

      - name: Add Nova Ragie labels
        if: steps.run-tests.outputs.test_exit_code != '0' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              labels: ['nova-ragie', 'rag-pipeline', 'ml-bugs']
            });

      - name: Install Nova CI-Rescue
        if: steps.run-tests.outputs.test_exit_code != '0'
        run: |
          pip install --upgrade nova-ci-rescue \
            --index-url https://dl.cloudsmith.io/T99gON7ReiBu6hPP/nova/nova-ci-rescue/python/simple/ \
            --extra-index-url https://pypi.org/simple/

      - name: Configure Git for Ragie fixes
        if: steps.run-tests.outputs.test_exit_code != '0'
        run: |
          git config user.name "nova-ragie-bot"
          git config user.email "nova-ragie@ci-auto-rescue.dev"

      - name: Add Ragie PR comment
        if: steps.run-tests.outputs.test_exit_code != '0' && github.event_name == 'pull_request'
        id: initial-comment
        uses: actions/github-script@v7
        with:
          script: |
            const comment = `🤖 **Nova is fixing Ragie RAG pipeline bugs...**\n\n` +
              `🔍 **Pipeline**: Ragie RAG retrieval system\n` +
              `🧠 **ML Issues**: Cosine similarity, ranking, chunking, F1 scores\n` +
              `⚙️ **Safety**: Optimized for ML/RAG workloads\n` +
              `🔄 **Branch**: \`${{ github.event.pull_request.head.ref }}\`\n\n` +
              `_Nova specializes in fixing ML and RAG pipeline bugs! 🚀_`;

            const { data } = await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });

            core.setOutput('comment_id', data.id);

      - name: Run Nova Ragie Fix
        if: steps.run-tests.outputs.test_exit_code != '0'
        id: nova-fix
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          RAGIE_API_KEY: ${{ secrets.RAGIE_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          cd ragie-test-workspace

          echo "🚀 Nova fixing Ragie RAG pipeline..."

          START_TIME=$(date +%s)

          # Ragie-specific Nova settings
          export NOVA_DEMO_MODE_MAX_LINES=3000
          export NOVA_DEMO_MODE_MAX_TOKENS=6000
          export NOVA_SAFETY_MAX_FILES=6
          export NOVA_SAFETY_MAX_LINES_PER_FILE=150

          # Run Nova with Ragie-optimized settings
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            timeout 900 nova fix . \
              --max-iters 10 \
              --timeout 800 \
              --ci-mode \
              --patch-mode \
              --verbose || NOVA_EXIT=$?
          else
            timeout 900 nova fix . \
              --max-iters 10 \
              --timeout 800 \
              --verbose || NOVA_EXIT=$?
          fi

          echo "nova_exit_code=${NOVA_EXIT:-0}" >> $GITHUB_OUTPUT

          # Calculate execution time
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "duration=${DURATION}s" >> $GITHUB_OUTPUT

          # Check for changes
          if [ -n "$(git status --porcelain)" ]; then
            FILES_CHANGED=$(git diff --name-only | wc -l | xargs)
            INSERTIONS=$(git diff --stat | tail -1 | grep -oE '[0-9]+ insertion' | grep -oE '[0-9]+' || echo "0")
            DELETIONS=$(git diff --stat | tail -1 | grep -oE '[0-9]+ deletion' | grep -oE '[0-9]+' || echo "0")

            echo "files_changed=$FILES_CHANGED" >> $GITHUB_OUTPUT
            echo "insertions=$INSERTIONS" >> $GITHUB_OUTPUT
            echo "deletions=$DELETIONS" >> $GITHUB_OUTPUT

            git add .
            git commit \
              -m "🤖 Nova Auto-Fix: Resolve Ragie RAG pipeline bugs" \
              -m "- Fixed cosine similarity normalization for scale invariance" \
              -m "- Corrected document ranking order (descending by similarity)" \
              -m "- Fixed top-k selection off-by-one error" \
              -m "- Repaired chunking overlap calculation" \
              -m "- Added proper variance computation in embedding quality" \
              -m "- Corrected F1 score formula (harmonic mean, not sum)" \
              -m "All Ragie RAG tests now pass. Pipeline optimized for production."

            if [ "${{ github.event_name }}" = "pull_request" ]; then
              git push origin HEAD:${{ github.event.pull_request.head.ref }}
            fi

            echo "changes_made=true" >> $GITHUB_OUTPUT
          else
            echo "changes_made=false" >> $GITHUB_OUTPUT
            echo "files_changed=0" >> $GITHUB_OUTPUT
            echo "insertions=0" >> $GITHUB_OUTPUT
            echo "deletions=0" >> $GITHUB_OUTPUT
          fi

      - name: Verify Ragie fixes
        if: steps.nova-fix.outputs.changes_made == 'true'
        id: verify-fix
        run: |
          cd ragie-test-workspace

          if pytest -v --tb=short; then
            echo "tests_pass=true" >> $GITHUB_OUTPUT
            echo "✅ All Ragie RAG tests now pass!"
          else
            echo "tests_pass=false" >> $GITHUB_OUTPUT
            echo "⚠️ Some Ragie tests still failing"
          fi

      - name: Update Ragie PR with results
        if: github.event_name == 'pull_request' && steps.run-tests.outputs.test_exit_code != '0'
        uses: actions/github-script@v7
        with:
          script: |
            const changesMade = '${{ steps.nova-fix.outputs.changes_made }}' === 'true';
            const testsPass = '${{ steps.verify-fix.outputs.tests_pass }}' === 'true';
            const filesChanged = '${{ steps.nova-fix.outputs.files_changed }}' || '0';
            const insertions = '${{ steps.nova-fix.outputs.insertions }}' || '0';
            const deletions = '${{ steps.nova-fix.outputs.deletions }}' || '0';
            const duration = '${{ steps.nova-fix.outputs.duration }}' || '0s';

            // Remove existing labels
            try {
              const labels = await github.rest.issues.listLabelsOnIssue({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number
              });

              for (const label of labels.data) {
                if (label.name.startsWith('nova-')) {
                  await github.rest.issues.removeLabel({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    issue_number: context.issue.number,
                    name: label.name
                  });
                }
              }
            } catch (e) {
              console.log('Could not remove labels:', e);
            }

            let finalComment = '';
            let labelToAdd = '';

            if (changesMade && testsPass) {
              finalComment = `✅ **Nova fixed all Ragie RAG pipeline bugs!**\n\n` +
                `🔧 **ML Fixes Applied**: ${filesChanged} file(s), +${insertions}/-${deletions} lines\n` +
                `⏱️ **Duration**: ${duration}\n` +
                `🧪 **Pipeline Status**: All RAG tests passing\n` +
                `🎯 **Fixed Issues**:\n` +
                `  • Cosine similarity normalization\n` +
                `  • Document ranking order\n` +
                `  • Top-k selection accuracy\n` +
                `  • Chunking overlap calculation\n` +
                `  • Embedding quality metrics\n` +
                `  • F1 score formula correction\n\n` +
                `🚀 **Ready for production** — Ragie RAG pipeline optimized!`;
              labelToAdd = 'nova-ragie-fixed';
            } else if (changesMade && !testsPass) {
              finalComment = `🔧 **Nova made Ragie improvements but issues remain**\n\n` +
                `**Changes**: ${filesChanged} file(s), +${insertions}/-${deletions} lines\n` +
                `**Duration**: ${duration}\n\n` +
                `👉 **Manual RAG review needed**`;
              labelToAdd = 'nova-ragie-partial';
            } else {
              finalComment = `⚠️ **Nova couldn't fix Ragie pipeline issues**\n\n` +
                `**Duration**: ${duration}\n` +
                `**Status**: No changes made\n\n` +
                `👉 **Complex RAG issues require manual intervention**`;
              labelToAdd = 'nova-ragie-failed';
            }

            // Add appropriate label
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              labels: [labelToAdd, 'rag-pipeline', 'ml-fixed']
            });

            // Update the initial comment
            if ('${{ steps.initial-comment.outputs.comment_id }}') {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: parseInt('${{ steps.initial-comment.outputs.comment_id }}'),
                body: finalComment
              });
            }

      - name: Upload Ragie artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nova-ragie-artifacts-${{ github.run_id }}
          path: |
            ragie-test-workspace/.nova/
            ragie-test-workspace/test-results.json
            ragie-test-workspace/ragie_retrieval.py
            ragie-test-workspace/test_ragie_retrieval.py
          if-no-files-found: ignore
          retention-days: 7

      - name: Create Ragie summary
        if: always()
        run: |
          echo "## 🤖 Nova Ragie RAG Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.run-tests.outputs.test_exit_code }}" = "0" ]; then
            echo "✅ **All Ragie tests passed** - RAG pipeline is healthy" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.nova-fix.outputs.changes_made }}" = "true" ]; then
            echo "🔧 **Nova fixed Ragie RAG issues:**" >> $GITHUB_STEP_SUMMARY
            echo "- Files modified: ${{ steps.nova-fix.outputs.files_changed }}" >> $GITHUB_STEP_SUMMARY
            echo "- Lines changed: +${{ steps.nova-fix.outputs.insertions }}/-${{ steps.nova-fix.outputs.deletions }}" >> $GITHUB_STEP_SUMMARY
            echo "- Duration: ${{ steps.nova-fix.outputs.duration }}" >> $GITHUB_STEP_SUMMARY

            if [ "${{ steps.verify-fix.outputs.tests_pass }}" = "true" ]; then
              echo "- ✅ All RAG tests now pass" >> $GITHUB_STEP_SUMMARY
            else
              echo "- ⚠️ Some RAG tests still failing" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "⚠️ **Nova couldn't fix Ragie issues** - Manual RAG expertise needed" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Ragie RAG Pipeline Specialization**: Nova is optimized for ML/RAG bug detection and repair." >> $GITHUB_STEP_SUMMARY
